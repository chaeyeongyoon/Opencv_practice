{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# face detection function\n",
    "def detect_face(src):\n",
    "    src_f = src.copy()\n",
    "    if src_f is None:\n",
    "        print('Image load failed!')\n",
    "        return\n",
    "    \n",
    "    classifier = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "    \n",
    "    if classifier.empty():\n",
    "        print('XML load failed')\n",
    "        return\n",
    "    \n",
    "    faces = classifier.detectMultiScale(src_f)\n",
    "    \n",
    "    for (x, y, w, h) in faces:\n",
    "        cv2.rectangle(src_f, (x, y), (x+w, y+h), (255, 0, 255), 2)\n",
    "        \n",
    "    cv2.imshow('face', src_f)\n",
    "    \n",
    "def detect_eyes(src):\n",
    "    src_e = src.copy()\n",
    "    \n",
    "    if src_e is None:\n",
    "        print('Image load failed!')\n",
    "        return\n",
    "    \n",
    "    eye_classifier = cv2.CascadeClassifier('haarcascade_eye.xml')\n",
    "    \n",
    "    if eye_classifier.empty():\n",
    "        print('XML load failed')\n",
    "        return\n",
    "    \n",
    "    eyes = eye_classifier.detectMultiScale(src_e)\n",
    "    \n",
    "    for (x, y, w, h) in eyes:\n",
    "        cv2.rectangle(src_e, (x, y), (x+w, y+h), (0, 0, 255), 2)\n",
    "        \n",
    "    cv2.imshow('eye', src_e)\n",
    "    \n",
    "def detect_face_eyes(src):\n",
    "    src_fe = src.copy()\n",
    "    \n",
    "    if src_fe is None:\n",
    "        print('Image load failed!')\n",
    "        return\n",
    "    face_classifier = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "    eye_classifier = cv2.CascadeClassifier('haarcascade_eye.xml')\n",
    "    \n",
    "    if eye_classifier.empty() or face_classifier.empty():\n",
    "        print('XML load failed')\n",
    "        return\n",
    "    \n",
    "    faces = face_classifier.detectMultiScale(src_fe)\n",
    "    \n",
    "    for (x1, y1, w1, h1) in faces:\n",
    "        cv2.rectangle(src_fe, (x1, y1), (x1+w1, y1+h1), (0, 0, 255), 2)\n",
    "        \n",
    "        face_ROI = src_fe[y1:y1+h1, x1:x1+w1]\n",
    "        eyes = eye_classifier.detectMultiScale(face_ROI)\n",
    "        \n",
    "        for (x2, y2, w2, h2) in eyes:\n",
    "            center = (int(x2+w2/2), int(y2+h2/2))\n",
    "            cv2.circle(face_ROI, center, int(w2/2), (255, 0, 0), 2, cv2.LINE_AA)\n",
    "    \n",
    "    cv2.imshow('face_eye', src_fe)\n",
    "    \n",
    "\n",
    "src = cv2.imread('./images/avengers.jpg', cv2.IMREAD_COLOR)\n",
    "if src is None:\n",
    "    print('Image load failed!')\n",
    "else:    \n",
    "    detect_face(src)\n",
    "    detect_eyes(src)\n",
    "    detect_face_eyes(src)\n",
    "    \n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gray변환시 향상 가능\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def detect_face_eyes(src):\n",
    "    src_fe = src.copy()\n",
    "    src_fe_gray = cv2.cvtColor(src_fe, cv2.COLOR_BGR2GRAY)\n",
    "    if src_fe is None:\n",
    "        print('Image load failed!')\n",
    "        return\n",
    "    face_classifier = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "    eye_classifier = cv2.CascadeClassifier('haarcascade_eye.xml')\n",
    "    \n",
    "    if eye_classifier.empty() or face_classifier.empty():\n",
    "        print('XML load failed')\n",
    "        return\n",
    "    \n",
    "    faces = face_classifier.detectMultiScale(src_fe_gray)\n",
    "    \n",
    "    for (x1, y1, w1, h1) in faces:\n",
    "        cv2.rectangle(src_fe, (x1, y1), (x1+w1, y1+h1), (0, 0, 255), 2)\n",
    "        \n",
    "        face_ROI = src_fe[y1:y1+h1, x1:x1+w1]\n",
    "        eyes = eye_classifier.detectMultiScale(face_ROI)\n",
    "        \n",
    "        for (x2, y2, w2, h2) in eyes:\n",
    "            center = (int(x2+w2/2), int(y2+h2/2))\n",
    "            cv2.circle(face_ROI, center, int(w2/2), (255, 0, 0), 2, cv2.LINE_AA)\n",
    "    \n",
    "    cv2.imshow('face_eye', src_fe)\n",
    "    \n",
    "\n",
    "src = cv2.imread('./images/avengers.jpg', cv2.IMREAD_COLOR)\n",
    "if src is None:\n",
    "    print('Image load failed!')\n",
    "else:    \n",
    "    detect_face_eyes(src)\n",
    "    \n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 영상에서 얼굴 인식 실행 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def detect_face_eyes():\n",
    "    ret, frame = cap.read()\n",
    "    frame_fe = frame.copy()\n",
    "    \n",
    "    face_classifier = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "    eye_classifier = cv2.CascadeClassifier('haarcascade_eye.xml')\n",
    "    \n",
    "    if eye_classifier.empty() or face_classifier.empty():\n",
    "        print('XML load failed')\n",
    "        return\n",
    "    \n",
    "    faces = face_classifier.detectMultiScale(frame_fe)\n",
    "    \n",
    "    for (x1, y1, w1, h1) in faces:\n",
    "        cv2.rectangle(frame_fe, (x1, y1), (x1+w1, y1+h1), (0, 0, 255), 2)\n",
    "        \n",
    "        face_ROI = frame_fe[y1:y1+h1, x1:x1+w1]\n",
    "        eyes = eye_classifier.detectMultiScale(face_ROI)\n",
    "        \n",
    "        for (x2, y2, w2, h2) in eyes:\n",
    "            center = (int(x2+w2/2), int(y2+h2/2))\n",
    "            cv2.circle(face_ROI, center, int(w2/2), (255, 0, 0), 2, cv2.LINE_AA)\n",
    "    \n",
    "    stacked = np.hstack((frame, frame_fe))\n",
    "    cv2.imshow('result',stacked)\n",
    "    \n",
    "        \n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "if cap.isOpened():\n",
    "    detect_face_eyes()\n",
    "else:\n",
    "    print('camera open error')\n",
    "    \n",
    "cv2.waitKey()       \n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "if cap.isOpened:\n",
    "    ret, frame = cap.read()\n",
    "    while ret:\n",
    "        frame_fe = frame.copy()\n",
    "   \n",
    "        face_classifier = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "        eye_classifier = cv2.CascadeClassifier('haarcascade_eye.xml')\n",
    "    \n",
    "        if eye_classifier.empty() or face_classifier.empty():\n",
    "            print('XML load failed')\n",
    "            break\n",
    "            \n",
    "        faces = face_classifier.detectMultiScale(frame_fe)\n",
    "  \n",
    "        for (x1, y1, w1, h1) in faces:\n",
    "            cv2.rectangle(frame_fe, (x1, y1), (x1+w1, y1+h1), (0, 0, 255), 2)\n",
    "        \n",
    "            face_ROI = frame_fe[y1:y1+h1, x1:x1+w1]\n",
    "            eyes = eye_classifier.detectMultiScale(face_ROI)\n",
    "      \n",
    "            for (x2, y2, w2, h2) in eyes:\n",
    "                center = (int(x2+w2/2), int(y2+h2/2))\n",
    "                cv2.circle(face_ROI, center, int(w2/2), (255, 0, 0), 2, cv2.LINE_AA)\n",
    "    \n",
    "        stacked = np.hstack((frame, frame_fe))\n",
    "        cv2.imshow('result',stacked)\n",
    "        \n",
    "        if cv2.waitKey() == 27:\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "face_classifier = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "eye_classifier = cv2.CascadeClassifier('haarcascade_eye.xml')\n",
    "    \n",
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 320)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 240)\n",
    "\n",
    "while True:\n",
    "    _, frame = cap.read()\n",
    "    frame_fe = frame.copy()\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_classifier.detectMultiScale(gray, 1.3, 3)\n",
    "    for (x1, y1, w1, h1) in faces:\n",
    "        cv2.rectangle(frame_fe, (x1, y1), (x1+w1, y1+h1), (0, 0, 255), 2)\n",
    "        \n",
    "        face_ROI = frame_fe[y1:y1+h1, x1:x1+w1]\n",
    "        eyes = eye_classifier.detectMultiScale(face_ROI)\n",
    "      \n",
    "        for (x2, y2, w2, h2) in eyes:\n",
    "            center = (int(x2+w2/2), int(y2+h2/2))\n",
    "            cv2.circle(face_ROI, center, int(w2/2), (255, 0, 0), 2, cv2.LINE_AA)\n",
    "    \n",
    "    stacked = np.hstack((frame, frame_fe))\n",
    "    cv2.imshow('result',stacked)\n",
    "    if cv2.waitKey() == 27:\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 강사님\n",
    "import cv2\n",
    "import numpy as np\n",
    "import sys\n",
    "face_classifier = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "eye_classifier = cv2.CascadeClassifier('haarcascade_eye.xml')\n",
    "    \n",
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 320)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 240)\n",
    "if not cap.isOpened():\n",
    "    print('camera open error')\n",
    "    sys.exit()\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print('Video open failed')\n",
    "        sys.exit()\n",
    "        \n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_classifier.detectMultiScale(gray, 1.3, 3)\n",
    "    for (x1, y1, w1, h1) in faces:\n",
    "        cv2.rectangle(frame, (x1, y1), (x1+w1, y1+h1), (0, 0, 255), 2)\n",
    "        \n",
    "        face_ROI = frame[y1:y1+h1, x1:x1+w1]\n",
    "        eyes = eye_classifier.detectMultiScale(face_ROI)\n",
    "      \n",
    "        for (x2, y2, w2, h2) in eyes:\n",
    "            center = (int(x2+w2/2), int(y2+h2/2))\n",
    "            cv2.circle(face_ROI, center, int(w2/2), (255, 0, 0), 2, cv2.LINE_AA)\n",
    "    \n",
    "    cv2.imshow('frame',frame)\n",
    "    if cv2.waitKey() == 27:\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 보행자검출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "\n",
    "cap = cv2.VideoCapture('./images/vtest.avi')\n",
    "fps = cap.get(cv2.CAP_PROP_FPS) # 또는 cap.get(5)\n",
    "width = cap.get(cv2.CAP_PROP_FRAME_WIDTH) # 또는 cap.get(3)\n",
    "height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT) # 또는 cap.get(4)\n",
    "#fourcc = cv2.VideoWriter_fourcc(*'DIVX') # 코덱 정의\n",
    "#out = cv2.VideoWriter('otter_out.avi', fourcc, fps, (int(width), int(height))) # VideoWriter 객체 정의\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print('Video open failed')\n",
    "    sys.exit()\n",
    "    \n",
    "hog = cv2.HOGDescriptor()\n",
    "hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    detected, _ = hog.detectMultiScale(frame)\n",
    "    \n",
    "    for(x, y, w, h) in detected:\n",
    "        c = (random.randint(0,255), random.randint(0,255), random.randint(0,255))\n",
    "        cv2.rectangle(frame, (x, y), (x+w,y+h), c, 3)\n",
    "        \n",
    "    cv2.imshow('frame', frame)\n",
    "    out.write(frame)\n",
    "    if cv2.waitKey(10) == 27:\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "#out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "oilpainting effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def minMaxFilterGray(img, kernel_size, flag):\n",
    "    kh = kw = kernel_size\n",
    "    kh2, kw2 = kh//2, kw//2\n",
    "    dst = np.zeros(img.shape, img.dtype)\n",
    "    #all pixel calculate\n",
    "    for i in range(dst.shape[0]):\n",
    "        for j in range(dst.shape[1]):\n",
    "            ROI = img[max(i-kh2,0):i+kh2+1, max(j-kw2,0):j+kw2+1]\n",
    "            minVal, maxVal,_,_ = cv2.minMaxLoc(ROI)\n",
    "            if flag == 0:\n",
    "                dst[i,j] = minVal\n",
    "            else:\n",
    "                dst[i,j] = maxVal\n",
    "    return dst\n",
    "\n",
    "def minMaxFilter(img, kernel_size, flag):\n",
    "    if len(img.shape) == 2:\n",
    "        return minMaxFilterGray(img, kernel_size, flag)\n",
    "    HSV = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "    H, S, V = cv2.split(HSV)\n",
    "    Vfiltered = minMaxFilterGray(V, kernel_size, flag)\n",
    "    HSVfiltered = cv2.merge((H,S, Vfiltered))\n",
    "    return cv2.cvtColor(HSVfiltered, cv2.COLOR_HSV2BGR)\n",
    "\n",
    "img = cv2.imread('./images/min_max.jpg')\n",
    "minFiltered = minMaxFilter(img, 7, 0)\n",
    "maxFiltered = minMaxFilter(img, 7, 1)\n",
    "\n",
    "cv2.imshow('img', img)\n",
    "cv2.imshow('minFiltered', minFiltered)\n",
    "cv2.imshow('maxFiltered', maxFiltered)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "non randering effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "edge_preserved:sigma_s, sigma_r= 60 0.4\n",
      "detail_enhanced:sigma_s, sigma_r= 60 0.4\n",
      "pencil_sketch:sigma_s, sigma_r= 60 0.4\n",
      "staylization:sigma_s, sigma_r= 60 0.4\n"
     ]
    }
   ],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np\n",
    "\n",
    "def on_ep_change(_):\n",
    "    sigma_s = cv.getTrackbarPos('sigma_s', 'edge_preserved')\n",
    "    sigma_r = cv.getTrackbarPos('sigma_r', 'edge_preserved') * 0.01\n",
    "    print('edge_preserved:sigma_s, sigma_r=',sigma_s, sigma_r)\n",
    "    dst = cv.edgePreservingFilter(img, sigma_s=sigma_s, sigma_r=sigma_r)\n",
    "    cv.imshow('edge_preserved', dst)\n",
    "    \n",
    "def on_de_change(_):\n",
    "    sigma_s = cv.getTrackbarPos('sigma_s', 'detail_enhanced')\n",
    "    sigma_r = cv.getTrackbarPos('sigma_r', 'detail_enhanced') * 0.01\n",
    "    print('detail_enhanced:sigma_s, sigma_r=',sigma_s, sigma_r)\n",
    "    dst = cv.detailEnhance(img, sigma_s=sigma_s, sigma_r=sigma_r)\n",
    "    cv.imshow('detail_enhanced', dst)\n",
    "    \n",
    "def on_ps_change(_):\n",
    "    sigma_s = cv.getTrackbarPos('sigma_s', 'pencil_sketch')\n",
    "    sigma_r = cv.getTrackbarPos('sigma_r', 'pencil_sketch') * 0.01\n",
    "    print('pencil_sketch:sigma_s, sigma_r=',sigma_s, sigma_r)\n",
    "    dst_gray, dst_color = cv.pencilSketch(img, sigma_s=sigma_s, sigma_r=sigma_r)\n",
    "    cv.imshow('pencil_sketch', dst_color)\n",
    "    cv.imshow('pencil_sketch_gray', dst_gray)\n",
    "    \n",
    "def on_st_change(_):\n",
    "    sigma_s = cv.getTrackbarPos('sigma_s', 'stylization')\n",
    "    sigma_r = cv.getTrackbarPos('sigma_r', 'stylization') * 0.01\n",
    "    print('staylization:sigma_s, sigma_r=',sigma_s, sigma_r)\n",
    "    dst = cv.stylization(img, sigma_s=sigma_s, sigma_r=sigma_r)\n",
    "    cv.imshow('stylization', dst)\n",
    "\n",
    "cv.namedWindow('edge_preserved')\n",
    "cv.createTrackbar('sigma_s', 'edge_preserved', 60, 60, on_ep_change)\n",
    "cv.createTrackbar('sigma_r', 'edge_preserved', 40, 100, on_ep_change)\n",
    "\n",
    "cv.namedWindow('detail_enhanced')\n",
    "cv.createTrackbar('sigma_s', 'detail_enhanced', 60, 60, on_de_change)\n",
    "cv.createTrackbar('sigma_r', 'detail_enhanced', 40, 100, on_de_change)\n",
    "\n",
    "cv.namedWindow('pencil_sketch')\n",
    "cv.createTrackbar('sigma_s', 'pencil_sketch', 60, 60, on_ps_change)\n",
    "cv.createTrackbar('sigma_r', 'pencil_sketch', 40, 100, on_ps_change)\n",
    "\n",
    "cv.namedWindow('stylization')\n",
    "cv.createTrackbar('sigma_s', 'stylization', 60, 60, on_st_change)\n",
    "cv.createTrackbar('sigma_r', 'stylization', 40, 100, on_st_change)\n",
    "\n",
    "img = cv.imread('red-and-green-leavs-of-autumn-jpg.jpg')\n",
    "on_ep_change(0)\n",
    "on_de_change(0)\n",
    "on_ps_change(0)\n",
    "on_st_change(0)\n",
    "cv.waitKey()\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "grabcut  \n",
    "영역을 지정해주면 나머지는 masking처리  \n",
    "백그라운드처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "\n",
    "src = cv.imread('./images/soccer_ball.jpg')\n",
    "mask= np.zeros(src.shape[:2], np.uint8) #mask\n",
    "rc = cv.selectROI(src)\n",
    "cv.grabCut(src, mask, rc, None, None, 5, cv.GC_INIT_WITH_RECT)\n",
    "mask2 = np.where((mask==0)| (mask==2), 0, 1).astype('uint8')\n",
    "\n",
    "dst = src * mask2[:,:,np.newaxis]\n",
    "cv.imshow('dst',dst)\n",
    "\n",
    "# mouse event function\n",
    "def on_mouse(event, x, y, flags, param):\n",
    "    # LBUTTONDOWN : foreground\n",
    "    if event == cv.EVENT_LBUTTONDOWN:\n",
    "        cv.circle(dst, (x, y), 3, (255, 0, 0), -1)\n",
    "        cv.circle(mask, (x, y), 3, cv.GC_FGD, -1)\n",
    "        cv.imshow('dst', dst)\n",
    "    # RBUTTONDOWN: background    \n",
    "    elif event == cv.EVENT_RBUTTONDOWN:\n",
    "        cv.circle(dst, (x, y), 3, (0, 0, 255), -1)\n",
    "        cv.circle(mask, (x, y), 3, cv.GC_BGD, -1)\n",
    "        cv.imshow('dst', dst)\n",
    "    # 왼오다눌려있을때지정\n",
    "    elif event == cv.EVENT_MOUSEMOVE:\n",
    "        if flags & cv.EVENT_FLAG_LBUTTON:\n",
    "            cv.circle(dst, (x, y), 3, (255, 0, 0), -1)\n",
    "            cv.circle(mask, (x, y), 3, cv.GC_FGD, -1)\n",
    "            cv.imshow('dst', dst)\n",
    "        elif flags & cv.EVENT_FLAG_RBUTTON:\n",
    "            cv.circle(dst, (x, y), 3, (0, 0, 255), -1)\n",
    "            cv.circle(mask, (x, y), 3, cv.GC_BGD, -1)\n",
    "            cv.imshow('dst', dst)\n",
    "\n",
    "cv.setMouseCallback('dst', on_mouse)\n",
    "\n",
    "while True:\n",
    "    key = cv.waitKey()\n",
    "    if key == 13: # enter\n",
    "        # 위에서 지정한 ROI에서 bgd, fgd Model을 이용하여 영상 분석\n",
    "        bgdModel = np.zeros((1,65),np.float64)\n",
    "        fgdModel = np.zeros((1,65),np.float64)\n",
    "\n",
    "        cv.grabCut(src, mask, rc, bgdModel, fgdModel, 1, cv.GC_INIT_WITH_MASK)\n",
    "        mask2 = np.where((mask==2)|(mask==0), 0, 1).astype('uint8')\n",
    "        dst = src * mask2[:,:,np.newaxis]\n",
    "        cv.imshow('dst', dst)\n",
    "    elif key == 27:\n",
    "        break\n",
    "        \n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 간단 스노우앱  \n",
    "얼굴이미지에 선글라스 합성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 3 is out of bounds for axis 2 with size 3",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-8871e092d2b4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m         \u001b[1;31m# overlay 함수로 영상 합성 : frame 영상에 glasses2 를 합성하는데, 시작 위치는 pos 부터이다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 100\u001b[1;33m         \u001b[0moverlay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglasses2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m     \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'frame'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-8871e092d2b4>\u001b[0m in \u001b[0;36moverlay\u001b[1;34m(img, glasses, pos)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[1;31m# glasses 에서 shape (h,w,4) 에서 마지막에 해당하는 부분을 alpha 값으로 지정할 수 있음\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m     \u001b[0malpha\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1.\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mglasses\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m255.\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# shape=(h, w)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;31m# BGR 채널별로 두 부분 영상의 가중합 : weighted sum 의 결과는 float32 이므로 uint8 로 바꿔야함\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 3 is out of bounds for axis 2 with size 3"
     ]
    }
   ],
   "source": [
    "# 얼굴에 안경 영상 합성하기\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# 3채널 img 영상에 4채널 item 영상을 pos 위치에 합성\n",
    "\n",
    "def overlay(img, glasses, pos):\n",
    "    # 실제 합성을 수행할 부분 영상 좌표 계산\n",
    "    # ex, ey 는 시작 좌표에 glasses 영상의 width, height 를 더한 값\n",
    "    sx = pos[0]\n",
    "    ex = pos[0] + glasses.shape[1]\n",
    "    sy = pos[1]\n",
    "    ey = pos[1] + glasses.shape[0]\n",
    "\n",
    "    # 합성할 영역이 입력 영상 크기를 벗어나면 예외처리\n",
    "    # ex, ey 는 시작 좌표에 glasses 영상의 width, height 를 더한 값이 영상의 크기를 넘는 경우 예외처리\n",
    "    if sx < 0 or sy < 0 or ex > img.shape[1] or ey > img.shape[0]:\n",
    "        return\n",
    "\n",
    "    # 부분 영상 참조. img1: 입력 영상의 부분 영상, img2: 안경 영상의 부분 영상\n",
    "    img1 = img[sy:ey, sx:ex]   # shape=(h, w, 3)\n",
    "    img2 = glasses[:, :, 0:3]  # glasses 는 shape 가 (h,w,4) 여서 shape=(h, w, 3) 으로 맞춰줌\n",
    "\n",
    "    # glasses 에서 shape (h,w,4) 에서 마지막에 해당하는 부분을 alpha 값으로 지정할 수 있음\n",
    "    alpha = 1. - (glasses[:, :, 3] / 255.)  # shape=(h, w)\n",
    "\n",
    "    # BGR 채널별로 두 부분 영상의 가중합 : weighted sum 의 결과는 float32 이므로 uint8 로 바꿔야함\n",
    "    img1[..., 0] = (img1[..., 0] * alpha + img2[..., 0] * (1. - alpha)).astype(np.uint8)\n",
    "    img1[..., 1] = (img1[..., 1] * alpha + img2[..., 1] * (1. - alpha)).astype(np.uint8)\n",
    "    img1[..., 2] = (img1[..., 2] * alpha + img2[..., 2] * (1. - alpha)).astype(np.uint8)\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 320)  # 프레임 폭을 480으로 설정\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 240)  # 프레임 높이를 320으로 설정\n",
    "\n",
    "w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "face_classifier = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "eye_classifier = cv2.CascadeClassifier('haarcascade_eye.xml')\n",
    "\n",
    "glasses = cv2.imread('./images/sunglass.png', cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "ew, eh = glasses.shape[:2]  # 가로, 세로 크기\n",
    "ex1, ey1 = 240, 300  # 안경 영상의 왼쪽 눈의 중심 (x,y) 좌표\n",
    "ex2, ey2 = 660, 300  # 안경 영상의 오른쪽 눈의 중심 (x,y) 좌표\n",
    "\n",
    "# 매 프레임에 대해 얼굴 검출 및 안경 합성\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # 얼굴 검출 : box 의 최대 최소 크기를 지정\n",
    "    faces = face_classifier.detectMultiScale(frame, scaleFactor=1.2, minSize=(100, 100), maxSize=(400, 400))\n",
    "\n",
    "    # (x,y) : 얼굴이 있는 왼쪽 상단의 (x,y) 좌표\n",
    "    for (x, y, w, h) in faces:\n",
    "        cv2.rectangle(frame, (x, y, w, h), (255, 0, 255), 2)\n",
    "\n",
    "        # 눈 검출\n",
    "        faceROI = frame[y:y + h // 2, x:x + w]\n",
    "        eyes = eye_classifier.detectMultiScale(faceROI)\n",
    "        # eyes[0][0] : 부분영상 faceROI 에서 첫 번째 눈의 x 좌표\n",
    "        # eyes[0][1] : 부분영상 faceROI 에서 첫 번째 눈의 y 좌표\n",
    "        # eyes[0][2] : 왼쪽눈의 가로크기\n",
    "        # eyes[0][3] : 왼쪽눈의 세로크기\n",
    "\n",
    "        # 눈을 2개 검출한 것이 아니라면 무시\n",
    "        if len(eyes) != 2: continue\n",
    "\n",
    "        # 두 개의 눈 중앙 위치를 (x1, y1), (x2, y2) 좌표로 저장\n",
    "        x1 = x + eyes[0][0] + (eyes[0][2] // 2)\n",
    "        y1 = y + eyes[0][1] + (eyes[0][3] // 2)\n",
    "        x2 = x + eyes[1][0] + (eyes[1][2] // 2)\n",
    "        y2 = y + eyes[1][1] + (eyes[1][3] // 2)\n",
    "\n",
    "        # x1 이 오른쪽 눈이면, (x1,y1) 과 (x2,y2) 를 바꾸자\n",
    "        if x1 > x2:\n",
    "            x1, y1, x2, y2 = x2, y2, x1, y1\n",
    "\n",
    "        #cv2.circle(faceROI, (x1, y1), 5, (255, 0, 0), 2, cv2.LINE_AA) # 확인용 검출선\n",
    "        #cv2.circle(faceROI, (x2, y2), 5, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "        # 두 눈 사이의 거리를 이용하여 스케일링 팩터를 계산 (두 눈이 수평하다고 가정)\n",
    "        # x2 - x1 : 영상에서 두 눈 사이의 거리\n",
    "        # ex2 - ex1 : 안경에서 안경알 사이의 거리\n",
    "        fx = (x2 - x1) / (ex2 - ex1)\n",
    "\n",
    "        # 실제 영상 크기를 안경 영상에서의 크기로 나눈 값으로 resizeing 하자\n",
    "        # 축소 변환에서 사용하는 INTER_AREA 사용\n",
    "        fx = (x2 - x1) / (ex2 - ex1)\n",
    "        glasses2 = cv2.resize(glasses, (0, 0), fx=fx, fy=fx, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "        # 크기 조절된 안경 영상을 합성할 위치 계산 (좌상단 좌표)\n",
    "        pos = (x1 - int(ex1 * fx), y1 - int(ey1 * fx))\n",
    "\n",
    "        # overlay 함수로 영상 합성 : frame 영상에 glasses2 를 합성하는데, 시작 위치는 pos 부터이다.\n",
    "        overlay(frame, glasses2, pos)\n",
    "\n",
    "    cv2.imshow('frame', frame)\n",
    "\n",
    "    if cv2.waitKey(1) == 27:break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
